---
title: "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"
excerpt: "ViT Paper Review"

categories:
  - Papers
tags:
  - [Computer Vision, ViT]
use_math: true

permalink: /papers/vit/

toc: true
toc_sticky: true

date: 2024-09-23
last_modified_at: 2024-09-23
---
# AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE

- ***Link :*** https://arxiv.org/abs/2010.11929

ì´ ë…¼ë¬¸ì—ì„œëŠ” NLPì—ì„œ ì‚¬ìš©ë˜ëŠ” Transformerë¥¼ ì´ìš©í•˜ì—¬ Image Classificationì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” Vision Transformer ëª¨ë¸ì„ ì†Œê°œí•œë‹¤.

### ğŸ’¡ **Model Architecture**  

![image](/assets/images/posts_img/vit/1.png)

ëª¨ë¸ì˜ êµ¬ì¡°ëŠ” ìœ„ì™€ ê°™ë‹¤.

![image](/assets/images/posts_img/vit/2.png)

ë¨¼ì € 2Dì´ë¯¸ì§€ë¥¼ 1ì°¨ì›ìœ¼ë¡œ(Flattening) Embeddingí•˜ê¸° ìœ„í•´, ì´ë¯¸ì§€ë¥¼ Patchë‹¨ìœ„ë¡œ ìª¼ê¹¨ëŠ” Patch Embeddingì„ ìˆ˜í–‰í•œë‹¤. 

Patch Embeddingì€ $x \in \mathbb{R}^{H \times W \times C}$ ì´ë¯¸ì§€ë¥¼ patch ë‹¨ìœ„ë¡œ ìª¼ê°œì–´, ê° patchê°€ $x_p \in \mathbb{R}^{N\times (P^2 \cdot C)}$ë˜ë„ë¡ ë§Œë“¤ì–´ì¤€ë‹¤.

- H: Height, W: Width, C: Channels
- $N=HW/P^2$

ì¶”ê°€ë¡œ, BERTì—ì„œ ì‚¬ìš©í•˜ëŠ” [Class] í† í°ê³¼ ë¹„ìŠ·í•˜ê²Œ Input Embeddingì˜ ë§¨ ì•ì— [Class] Patchë¥¼ ë„£ì–´ì¤€ë‹¤. [Class] íŒ¨ì¹˜ëŠ” Transformer Encoderì˜ ì¶œë ¥($z_l$
)ì˜ ë§¨ ì•($z_l^0$)ì— ëŒ€ì‘ë˜ë©° ì´í›„ MLPì˜ inputìœ¼ë¡œ ë“¤ì–´ê°€ classificationì— ì‚¬ìš©ëœë‹¤.

ë˜í•œ ê° patchë“¤ì˜ ìˆœì„œì •ë³´ë¥¼ ë„£ì–´ì£¼ê¸° ìœ„í•´ Positional Encodingì„ ê° Patch Embeddingì— ë”í•´ì¤€ë‹¤.

![image](/assets/images/posts_img/vit/3.png)

Positional Encodingì€ ìœ„ ì‚¬ì§„ ì¤‘ ê°€ìš´ë°ì˜ ëª¨ìŠµì²˜ëŸ¼, ìì‹ ì˜ patchê°€ ê°€ì¥ í™œì„±í™”ë˜ì–´ìˆëŠ” ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆë‹¤.( (1,1) patchëŠ” ì™¼ìª½ ë§¨ìœ„ ëª¨ì„œë¦¬ê°€ ê°€ì¥ í™œì„±í™” ë˜ì–´ìˆìŒ) ì´ì™€ ê°™ì€ positional encodingì„ patch embeddingì— ë”í•´ì¤˜ ìœ„ì¹˜ê°’ì„ ë³´ì¡´í•  ìˆ˜ ìˆë‹¤.

ì´ë ‡ê²Œ êµ¬í•´ì§„ ë²¡í„°ë¥¼ Transformer Encoderì˜ inputìœ¼ë¡œ ë„£ì–´ì£¼ê³ , Transformer Encoderë¡œë¶€í„° ì¶œë ¥í•œ Outputì„ MLPì˜ inputìœ¼ë¡œ ë„£ì–´ image classificationì„ ìˆ˜í–‰í•œë‹¤.

ì´ë•Œ, ì—¬ê¸°ì„œ ì‚¬ìš©í•œ Transformer EncoderëŠ” ê¸°ì¡´ì˜ Transformerì˜ Encoderì™€ëŠ” êµ¬ì¡°ê°€ ì¡°ê¸ˆ ë‹¤ë¥´ë‹¤.

![image](/assets/images/posts_img/vit/4.png)

Transforemr - Attention Is All You Need

ì—¬ê¸°ì„œ ì‚¬ìš©í•œ Transformer EncoderëŠ” Normalizationì„ ë¨¼ì € ìˆ˜í–‰í•´ì£¼ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

ìœ„ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

![image](/assets/images/posts_img/vit/5.png)

ViTë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„

- MSA: Multi-Head Self-Attention
- LN: Layer-Normalization
- $Z'_l, Z_l$ì—ì„œ residual connection($+z_{l-1},z'_l$) ì‚¬ìš©

### ğŸ’¡ **Hybrid Architecture**  

ì´ë¯¸ì§€ë¥¼ Patchë‹¨ìœ„ë¡œ ìª¼ê°œì–´ Linear Projectionì„ í†µí•´ Embedding Vectorë¡œ ë§Œë“œëŠ” ëŒ€ì‹ ì—, CNNì„ ì´ìš©í•˜ì—¬ êµ¬í•œ Feature Mapì„ Transformerì˜ input vectorìœ¼ë¡œ ë„£ì–´ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤.

### ğŸ’¡ **Result**  

![image](/assets/images/posts_img/vit/6.png)

ê° ì´ë¯¸ì§€ì˜ ì ì ˆí•œ ë¶€ë¶„ì´ í™œì„±í™”ëœ ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆë‹¤.

### ğŸ’¡ **Self-Supervision**  

BERTì—ì„œ self-supervised pre-trainingì„ í•œ ê²ƒì²˜ëŸ¼, ViTì—ì„œë„ Patchì˜ ì¼ë¶€ë¥¼ maskingí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì„±ëŠ¥ì„ í–¥ìƒ ì‹œí‚¤ë ¤ëŠ” ì‹œë„ë¥¼ í–ˆë‹¤.